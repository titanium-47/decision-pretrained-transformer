import argparse
import os
import random
import time

import numpy as np
import torch
import wandb

import common_args
from envs.gpu_bandit_env import GPUBanditEnv
from models.net import Transformer

device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')


def set_seeds(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)


def build_bandit_env(dim, n_envs, horizon, var, env_name):
    if env_name == 'bandit':
        bandit_type = 'uniform'
    elif env_name == 'bandit_thompson':
        bandit_type = 'bernoulli'
    else:
        raise ValueError(f"Unsupported env: {env_name}")
    env = GPUBanditEnv(dim, n_envs, horizon, var=var, type=bandit_type, device=device)
    return env

def cross_entropy_loss(logits, target, reduction='none'):
    return torch.nn.functional.cross_entropy(logits, target, reduction=reduction)

if __name__ == '__main__':
    if not os.path.exists('trained_models'):
        os.makedirs('trained_models', exist_ok=True)

    parser = argparse.ArgumentParser()
    common_args.add_dataset_args(parser)
    common_args.add_model_args(parser)
    common_args.add_train_args(parser)
    parser.add_argument('--seed', type=int, default=0)
    parser.add_argument('--K', type=int, default=-1, help='Interactive rollout steps')
    parser.add_argument('--episodes_per_epoch', type=int, default=1000)
    parser.add_argument('--beta', type=float, default=1.0, help='Beta for advantage weighting')

    args = vars(parser.parse_args())
    print("Args: ", args)

    env_name = args['env']
    if env_name not in ['bandit', 'bandit_thompson']:
        raise ValueError("train_interactive.py currently supports bandit envs only.")

    horizon = args['H']
    K = args['K'] if args['K'] > 0 else horizon
    dim = args['dim']
    state_dim = 1
    action_dim = dim
    n_embd = args['embd']
    n_layer = args['layer']
    n_head = args['head']
    lr = args['lr']
    dropout = args['dropout']
    var = args['var']
    num_epochs = args['num_epochs']
    seed = args['seed']
    episodes_per_epoch = args['episodes_per_epoch']
    n_envs = args['envs']
    beta = args['beta']

    tmp_seed = 0 if seed == -1 else seed
    set_seeds(tmp_seed)

    config = {
        'horizon': K,
        'state_dim': state_dim,
        'action_dim': action_dim,
        'n_layer': n_layer,
        'n_embd': n_embd,
        'n_head': n_head,
        'dropout': dropout,
        'test': False,
    }
    wandb_config = {**config, 'lr': lr, 'num_epochs': num_epochs, 'episodes_per_epoch': episodes_per_epoch, 'beta': beta, 'seed': tmp_seed, 'env': env_name}
    wandb.init(project='decision-pretrained-transformer', config=wandb_config)

    model_explorer = Transformer(config).to(device)
    model_exploiter = Transformer(config).to(device)
    optimizer_explorer = torch.optim.AdamW(model_explorer.parameters(), lr=lr, weight_decay=1e-4)
    optimizer_exploiter = torch.optim.AdamW(model_exploiter.parameters(), lr=lr, weight_decay=1e-4)
    loss_fn = torch.nn.CrossEntropyLoss()

    zeros = torch.zeros((n_envs, state_dim ** 2 + action_dim + 1), device=device)

    for epoch in range(num_epochs):
        model_explorer.train()
        model_exploiter.train()
        epoch_loss_explorer = 0.0
        epoch_loss_exploiter = 0.0
        start_time = time.time()

        for _ in range(episodes_per_epoch):
            with torch.no_grad():
                env = build_bandit_env(dim, n_envs, K, var, env_name)
                state = env.reset()

                context_states = torch.zeros((n_envs, K, state_dim), device=device)
                context_actions = torch.zeros((n_envs, K, action_dim), device=device)
                context_next_states = torch.zeros((n_envs, K, state_dim), device=device)
                context_rewards = torch.zeros((n_envs, K, 1), device=device)
                advantages = torch.zeros((n_envs, K-1, 1), device=device)
                previous_loss = None

                target = env.opt_a_index.to(device)
                # fill replay buffer
                for t in range(K):
                    query_states = state.float().view(n_envs, state_dim)
                    batch = {
                        'context_states': context_states[:, :t, :],
                        'context_actions': context_actions[:, :t, :],
                        'context_next_states': context_next_states[:, :t, :],
                        'context_rewards': context_rewards[:, :t, :],
                        'query_states': query_states,
                        'zeros': zeros,
                    }

                    # explorer inference
                    explore_logits = model_explorer(batch)[:, -1, :]
                    explore_action_dist = torch.distributions.Categorical(logits=explore_logits)
                    explore_action_idx = explore_action_dist.sample()
                    action = torch.nn.functional.one_hot(
                        explore_action_idx, num_classes=action_dim).float().to(device)
                    random_action = torch.randint(0, action_dim, (n_envs,))
                    random_action = torch.nn.functional.one_hot(
                        random_action, num_classes=action_dim).float().to(device) 
                    
                    # exploiter inference
                    exploiter_logits = model_exploiter(batch)[:, -1, :]
                    exploiter_action_dist = torch.distributions.Categorical(logits=exploiter_logits)
                    exploiter_action_idx = exploiter_action_dist.sample()
                    exploiter_action = torch.nn.functional.one_hot(
                        exploiter_action_idx, num_classes=action_dim).float().to(device)
                    
                    # step the environment with the random action
                    next_state, reward, _, _ = env.step(random_action.detach())

                    context_states[:, t, :] = state.float()
                    context_actions[:, t, :] = action
                    context_next_states[:, t, :] = next_state.float()
                    context_rewards[:, t, 0] = reward.float()

                    state = next_state
                    current_loss = cross_entropy_loss(exploiter_logits, target, reduction='none')
                    # the advantage of the previous exploration action
                    # is the difference between the current loss and the previous loss of the exploiter
                    if t > 0:
                        advantages[:, t-1] = (current_loss - previous_loss).unsqueeze(-1)

                    previous_loss = current_loss

            # update exploiter

            exploiter_logits = model_exploiter(batch)
            B, S, A = exploiter_logits.shape
            target = target.unsqueeze(-1).expand(B, S)
            exploiter_loss = loss_fn(exploiter_logits.reshape(-1, A), target.reshape(-1))
            optimizer_exploiter.zero_grad()
            exploiter_loss.backward()
            optimizer_exploiter.step()

            # update explorer
            explorer_logits = model_explorer(batch)[:, :-1, :] 
            B, S, A = explorer_logits.shape
            actions = torch.argmax(batch['context_actions'], dim=-1)
            advantage_weight = torch.exp(advantages * (1/beta)).detach()
            explorer_loss = cross_entropy_loss(explorer_logits.reshape(-1, A), actions.reshape(-1), reduction='none') 
            explorer_loss = explorer_loss.reshape(B, S) * advantage_weight.squeeze(-1)
            explorer_loss = explorer_loss.mean()

            optimizer_explorer.zero_grad()
            explorer_loss.backward()
            optimizer_explorer.step()

            epoch_loss_explorer += explorer_loss.item()
            epoch_loss_exploiter += exploiter_loss.item()

        avg_explorer_loss = epoch_loss_explorer / max(1, episodes_per_epoch)
        avg_exploiter_loss = epoch_loss_exploiter / max(1, episodes_per_epoch)
        elapsed = time.time() - start_time
        wandb.log({
            'explorer_loss': avg_explorer_loss,
            'exploiter_loss': avg_exploiter_loss,
            'epoch_time': elapsed,
        }, step=epoch + 1)
        print(f"Epoch {epoch + 1}/{num_epochs} - explorer_loss: {avg_explorer_loss:.6f} - exploiter_loss: {avg_exploiter_loss:.6f} - time: {elapsed:.2f}s")

    torch.save(model_explorer.state_dict(), 'trained_models/explorer_bandit.pt')
    torch.save(model_exploiter.state_dict(), 'trained_models/exploiter_bandit.pt')
    print("Done.")
